{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1ca7f31e9b0>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from UNet import ConvBlock, Encoder, Decoder, UNet\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2., 2.],\n",
       "         [1., 4.]]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randint(5, (1, 2, 2), dtype=torch.float32)\n",
    "input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight & Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "tconv_layer = nn.ConvTranspose2d(\n",
    "    in_channels=1, out_channels=1, kernel_size=2, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[1., 1.],\n",
       "          [1., 1.]]]], requires_grad=True)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tconv_layer.weight.data = torch.ones(\n",
    "    tconv_layer.weight.data.shape\n",
    ")\n",
    "tconv_layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "tconv_layer.bias = nn.Parameter(\n",
    "    torch.tensor([1], dtype=torch.float32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.,  5.,  3.],\n",
       "         [ 4., 10.,  7.],\n",
       "         [ 2.,  6.,  5.]]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tconv_layer(input)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "tconv_layer = nn.ConvTranspose2d(\n",
    "    in_channels=1, out_channels=1, kernel_size=2, padding=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[1., 1.],\n",
       "          [1., 1.]]]], requires_grad=True)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tconv_layer.weight.data = torch.ones(\n",
    "    tconv_layer.weight.data.shape\n",
    ")\n",
    "tconv_layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[9.0677]]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tconv_layer(input)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3., 9., 6.]]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tconv_layer = nn.ConvTranspose2d(\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    kernel_size=2,\n",
    "    padding=(1, 0),\n",
    "    bias=False\n",
    ")\n",
    "\n",
    "tconv_layer.weight.data = torch.ones(\n",
    "    tconv_layer.weight.data.shape\n",
    ")\n",
    "\n",
    "output = tconv_layer(input)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2.],\n",
       "         [1., 1., 4., 4.],\n",
       "         [1., 1., 4., 4.]]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tconv_layer = nn.ConvTranspose2d(\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    kernel_size=2,\n",
    "    stride=2,\n",
    "    padding=0,\n",
    "    bias=False\n",
    ")\n",
    "\n",
    "tconv_layer.weight.data = torch.ones(\n",
    "    tconv_layer.weight.data.shape\n",
    ")\n",
    "\n",
    "output = tconv_layer(input)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2., 2.],\n",
       "         [1., 4.]]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tconv_layer = nn.ConvTranspose2d(\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    kernel_size=2,\n",
    "    stride=2,\n",
    "    padding=1,\n",
    "    bias=False\n",
    ")\n",
    "\n",
    "tconv_layer.weight.data = torch.ones(\n",
    "    tconv_layer.weight.data.shape\n",
    ")\n",
    "\n",
    "output = tconv_layer(input)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConvBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randint(5, (1, 1, 256, 256), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvBlock(\n",
       "  (conv_block): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_conv = ConvBlock(in_channels=1, out_channels=64)\n",
    "in_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 256, 256])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = in_conv(input)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 128, 128])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder(in_channels=64, out_channels=128)\n",
    "input = torch.randint(5, (1, 64, 256, 256), dtype=torch.float32)\n",
    "output = encoder(input)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(n_channels=1, n_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Input Conv: torch.Size([4, 64, 256, 256])\n",
      "Shape Encoder 1: torch.Size([4, 128, 128, 128])\n",
      "Shape Encoder 2: torch.Size([4, 256, 64, 64])\n",
      "Shape Encoder 3: torch.Size([4, 512, 32, 32])\n",
      "Shape Encoder 4: torch.Size([4, 1024, 16, 16])\n",
      "Shape Decoder 1: torch.Size([4, 512, 32, 32])\n",
      "Shape Decoder 2: torch.Size([4, 256, 64, 64])\n",
      "Shape Decoder 3: torch.Size([4, 128, 128, 128])\n",
      "Shape Decoder 4: torch.Size([4, 64, 256, 256])\n",
      "Shape Output decoder: torch.Size([4, 2, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "input = torch.randint(\n",
    "    5, (4, 1, 256, 256), dtype=torch.float32\n",
    ")\n",
    "\n",
    "predictions = model(input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aio_exercise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
