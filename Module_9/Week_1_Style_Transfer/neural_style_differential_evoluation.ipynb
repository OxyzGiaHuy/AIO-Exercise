{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gzgGu4IN-MB",
        "outputId": "f222bd7e-ab41-4e46-a4d4-dfac1438883d"
      },
      "outputs": [],
      "source": [
        "!gdown 1mU--DNhy8pWMTljj7jI3FvJwRYRHwAq5\n",
        "!gdown 13XpLuVuxI6ekdEf5UElKH_IMWrK8wZU1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyZxAhvd6i85"
      },
      "source": [
        "## 1. Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkxOtv6w6i86"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S15oJOFG6i87"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvnMQ2BS6i87"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrjwG_Ta6i87"
      },
      "source": [
        "## 2. Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmI6sEcm6i87"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "imsize = 256\n",
        "\n",
        "\n",
        "img_transforms = transforms.Compose([\n",
        "    transforms.Resize((imsize, imsize)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qn4d8mqo6i88"
      },
      "outputs": [],
      "source": [
        "def image_loader(image_name):\n",
        "    image = Image.open(image_name)\n",
        "    image = img_transforms(image).unsqueeze(0)\n",
        "    return image.to(device, torch.float)\n",
        "\n",
        "style_img = image_loader(\"style_img2.jpg\")\n",
        "content_img = image_loader(\"content_img.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2eN-LC16i88"
      },
      "outputs": [],
      "source": [
        "unloader = transforms.ToPILImage()\n",
        "\n",
        "def imshow(tensor, title=None):\n",
        "    image = tensor.cpu().clone()\n",
        "    image = image.squeeze(0)\n",
        "    image = unloader(image)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(image)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "imshow(style_img, title='Style Image')\n",
        "\n",
        "plt.figure()\n",
        "imshow(content_img, title='Content Image')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u9b-jom6i88"
      },
      "source": [
        "## 3. Loss Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_mxXpUQ6i89"
      },
      "source": [
        "### 3.1 Content Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_vTXTmw6i89"
      },
      "outputs": [],
      "source": [
        "content_weight = 1.0\n",
        "ContentLoss = nn.MSELoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyyYHCUq6i89"
      },
      "source": [
        "### 3.2 Style Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WFkS14J6i89"
      },
      "outputs": [],
      "source": [
        "def gram_matrix(tensor):\n",
        "    a, b, c, d = tensor.size()\n",
        "    tensor = tensor.view(a * b, c * d)\n",
        "    G = torch.mm(tensor, tensor.t())\n",
        "    return G.div(a * b * c * d)\n",
        "\n",
        "style_weight = 1e6\n",
        "StyleLoss = nn.MSELoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZovf7JT6i89"
      },
      "source": [
        "## 3. Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIZ-08Bz6i89",
        "outputId": "3f5222eb-e1a8-4067-f966-54921e11f7be"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import vgg19, VGG19_Weights\n",
        "\n",
        "VGG19_pretrained = vgg19(weights=VGG19_Weights.DEFAULT).features.eval()\n",
        "VGG19_pretrained.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xLhurf4Xy17",
        "outputId": "9d926746-feb3-4968-e9ed-8b56454c187e"
      },
      "outputs": [],
      "source": [
        "class Normalization(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Normalization, self).__init__()\n",
        "        self.mean = torch.tensor(torch.tensor([0.485, 0.456, 0.406]).to(device)).view(-1, 1, 1)\n",
        "        self.std = torch.tensor(torch.tensor([0.229, 0.224, 0.225]).to(device)).view(-1, 1, 1)\n",
        "\n",
        "    def forward(self, img):\n",
        "        return (img - self.mean) / self.std\n",
        "\n",
        "normalization = Normalization().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4bX3Xw_6i89"
      },
      "outputs": [],
      "source": [
        "content_layers = ['conv_4']\n",
        "style_layers = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EFoHggz6i89"
      },
      "outputs": [],
      "source": [
        "def get_features(pretrained_model, image):\n",
        "    layers = {\n",
        "        '0': 'conv_1',\n",
        "        '5': 'conv_2',\n",
        "        '16': 'conv_3',\n",
        "        '25': 'conv_4',\n",
        "        '34': 'conv_5'\n",
        "    }\n",
        "    features = {}\n",
        "    x = image\n",
        "    x = normalization(x)\n",
        "    for name, pretrained_layer in pretrained_model._modules.items():\n",
        "        x = pretrained_layer(x)\n",
        "        if name in layers:\n",
        "            features[layers[name]] = x\n",
        "    return features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_MZZ-xlN-MH"
      },
      "outputs": [],
      "source": [
        "def rot_style_features(style_features, style_layers):\n",
        "    final_rot_style_features = {}\n",
        "    for layer in style_layers:\n",
        "        sf = style_features[layer].clone()\n",
        "        ################################### YOUR CODE HERE ##############################\n",
        "        # 2. Rotate the cloned tensor 90 degrees in the spatial dimensions (2, 3).\n",
        "        # 3. Rotate the 90-degree rotated tensor another 90 degrees (180 degrees total).\n",
        "        # 4. Calculate the final rotated feature by adding the original feature\n",
        "        # to the difference between the 90-degree and 180-degree rotations.\n",
        "        ################################################################################\n",
        "        final_rot_style_features[layer] = final_rot\n",
        "    return final_rot_style_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfH9P49i6i89"
      },
      "outputs": [],
      "source": [
        "content_features = get_features(VGG19_pretrained, content_img)\n",
        "style_features1 = get_features(VGG19_pretrained, style_img)\n",
        "final_rot_style_features = rot_style_features(style_features1, style_layers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VgFr8cm6i89"
      },
      "source": [
        "## 4. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYFF__he6i89"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "target_img1 = content_img.clone().requires_grad_(True).to(device)\n",
        "target_img2 = content_img.clone().requires_grad_(True).to(device)\n",
        "optimizer1 = optim.Adam([target_img1], lr=0.02)\n",
        "optimizer2 = optim.Adam([target_img2], lr=0.02)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trqDgkygN-MI"
      },
      "outputs": [],
      "source": [
        "\n",
        "def style_tranfer_(model, optimizer, target_img,\n",
        "                   content_features, style_features,\n",
        "                   style_layers, content_weight, style_weight):\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    with torch.no_grad():\n",
        "        target_img.clamp_(0, 1)\n",
        "    target_features = get_features(model, target_img)\n",
        "\n",
        "    content_loss = ContentLoss(content_features['conv_4'], target_features['conv_4'])\n",
        "\n",
        "    style_loss = 0\n",
        "    for layer in style_layers:\n",
        "        target_gram = gram_matrix(target_features[layer])\n",
        "        style_gram = gram_matrix(style_features[layer])\n",
        "        style_loss += StyleLoss(style_gram, target_gram)\n",
        "\n",
        "    total_loss = content_loss*content_weight + style_loss*style_weight\n",
        "    total_loss.backward(retain_graph=True)\n",
        "    optimizer.step()\n",
        "    return total_loss, content_loss, style_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UllrF3V6i89",
        "outputId": "1fab96d8-c08c-484c-fa38-3ca031f325af"
      },
      "outputs": [],
      "source": [
        "STEPS = 500\n",
        "\n",
        "for step in range(STEPS):\n",
        "\n",
        "    total_loss1, content_loss1, style_loss1 = style_tranfer_(VGG19_pretrained, optimizer1, target_img1,\n",
        "                                                           content_features, style_features1,\n",
        "                                                           style_layers, content_weight, style_weight)\n",
        "\n",
        "    total_loss2, content_loss2, style_loss2 = style_tranfer_(VGG19_pretrained, optimizer2, target_img2,\n",
        "                                                           content_features, final_rot_style_features,\n",
        "                                                           style_layers, content_weight, style_weight)\n",
        "\n",
        "    if step % 100 == 99:\n",
        "        print(f\"Epoch [{step+1}/{STEPS}] Total loss1: {total_loss1.item():.6f} - \\\n",
        "                Content loss1: {content_loss1.item():.6f} - Style loss1: {style_loss1.item():.6f}\")\n",
        "        print(f\"Epoch [{step+1}/{STEPS}] Total loss2: {total_loss2.item():.6f} - \\\n",
        "                Content loss1: {content_loss2.item():.6f} - Style loss2: {style_loss2.item():.6f}\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        target_img1.clamp_(0, 1)\n",
        "        target_img2.clamp_(0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5r1i6wU6i8-"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.axis('off')\n",
        "\n",
        "imshow(target_img1.detach(), title='Output Image1')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dN0pikYmN-MI"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.axis('off')\n",
        "\n",
        "imshow(target_img2.detach(), title='Output Image2')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "aio_exercise",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
