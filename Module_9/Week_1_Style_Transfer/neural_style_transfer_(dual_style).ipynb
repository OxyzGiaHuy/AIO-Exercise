{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9QnDjRSNYac",
        "outputId": "2b252c5f-f6dd-4c03-b6f2-086d61d9e09e"
      },
      "outputs": [],
      "source": [
        "!gdown 1mU--DNhy8pWMTljj7jI3FvJwRYRHwAq5\n",
        "!gdown 1yt9RDch0ZT9gtTfEBQLFFhGPw3PSOEh0\n",
        "!gdown 13XpLuVuxI6ekdEf5UElKH_IMWrK8wZU1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyZxAhvd6i85"
      },
      "source": [
        "## 1. Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkxOtv6w6i86"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S15oJOFG6i87"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvnMQ2BS6i87"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrjwG_Ta6i87"
      },
      "source": [
        "## 2. Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmI6sEcm6i87"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "imsize = 256\n",
        "\n",
        "img_transforms = transforms.Compose([\n",
        "    transforms.Resize((imsize, imsize)),\n",
        "    transforms.ToTensor()\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qn4d8mqo6i88"
      },
      "outputs": [],
      "source": [
        "def image_loader(image_name):\n",
        "    image = Image.open(image_name)\n",
        "    image = img_transforms(image).unsqueeze(0) # add new dim for batch size (index 0)\n",
        "    return image.to(device, torch.float)\n",
        "\n",
        "style_img1 = image_loader(\"style_img.jpg\")\n",
        "style_img2 = image_loader(\"style_img2.jpg\")\n",
        "content_img = image_loader(\"content_img.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "S2eN-LC16i88",
        "outputId": "bc55146b-f40a-4855-fbaf-12a1047ea6bc"
      },
      "outputs": [],
      "source": [
        "unloader = transforms.ToPILImage() # to use imshow\n",
        "\n",
        "def imshow(tensor, title=None):\n",
        "    image = tensor.cpu().clone()\n",
        "    image = image.squeeze(0)\n",
        "    image = unloader(image)\n",
        "    plt.imshow(image)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "imshow(style_img1, title='Style Image1')\n",
        "\n",
        "plt.figure()\n",
        "imshow(style_img2, title='Style Image2')\n",
        "\n",
        "plt.figure()\n",
        "imshow(content_img, title='Content Image')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u9b-jom6i88"
      },
      "source": [
        "## 3. Loss Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_mxXpUQ6i89"
      },
      "source": [
        "### 3.1 Content Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_vTXTmw6i89"
      },
      "outputs": [],
      "source": [
        "content_weight = 1\n",
        "ContentLoss = nn.MSELoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyyYHCUq6i89"
      },
      "source": [
        "### 3.2 Style Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WFkS14J6i89"
      },
      "outputs": [],
      "source": [
        "def gram_matrix(tensor):\n",
        "    \"\"\"\n",
        "    Calculate the Gram Matrix of a given 4D tensor of shape [B, C, H, W].\n",
        "    This is commonly used in style transfer to capture correlations between feature maps (style representation)\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: The normalized Gram matrix of shape [B*C, B*C]\n",
        "    \"\"\"\n",
        "    B, C, H, W = tensor.size()\n",
        "    tensor = tensor.view(B * C, H * W)\n",
        "    G = torch.mm(tensor, tensor.t())\n",
        "    return G.div(B * C * H * W)\n",
        "\n",
        "style_weight = 1e6\n",
        "StyleLoss = nn.MSELoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZovf7JT6i89"
      },
      "source": [
        "## 3. Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIZ-08Bz6i89",
        "outputId": "c3569597-9b3d-4d25-8b8d-0ad4c0abfdf8"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import vgg19, VGG19_Weights\n",
        "\n",
        "VGG19_pretrained = vgg19(weights=VGG19_Weights.DEFAULT).features.eval()\n",
        "VGG19_pretrained.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dS9ktoyaWo_g",
        "outputId": "d1a9c3d9-e106-481d-b53e-5d20e3cf961c"
      },
      "outputs": [],
      "source": [
        "class Normalization(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Normalization, self).__init__()\n",
        "        self.mean = torch.tensor(torch.tensor([0.485, 0.456, 0.406]).to(device)).view(-1, 1, 1)\n",
        "        self.std = torch.tensor(torch.tensor([0.229, 0.224, 0.225]).to(device)).view(-1, 1, 1)\n",
        "\n",
        "    def forward(self, img):\n",
        "        return (img - self.mean) / self.std\n",
        "\n",
        "normalization = Normalization().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4bX3Xw_6i89"
      },
      "outputs": [],
      "source": [
        "content_layers = ['conv_4']\n",
        "style_layers = ['conv_1', 'conv_2',\n",
        "                'conv_3', 'conv_4',\n",
        "                'conv_5']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EFoHggz6i89"
      },
      "outputs": [],
      "source": [
        "def get_features(pretrained_model, image):\n",
        "    layers = {\n",
        "        '0': 'conv_1',\n",
        "        '5': 'conv_2',\n",
        "        '10': 'conv_3',\n",
        "        '19': 'conv_4',\n",
        "        '28': 'conv_5'\n",
        "    }\n",
        "    features = {}\n",
        "    x = image\n",
        "    x = normalization(x)\n",
        "    for name, pretrained_layer in pretrained_model._modules.items():\n",
        "        x = pretrained_layer(x)\n",
        "        if name in layers:\n",
        "            features[layers[name]] = x\n",
        "    return features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1eNRHOnNNYas"
      },
      "outputs": [],
      "source": [
        "def get_dual_style(style_features1, style_features2, style_layers):\n",
        "    final_style_features = {}\n",
        "    for layer in style_layers:\n",
        "        sf1 = style_features1[layer]\n",
        "        sf2 = style_features2[layer]\n",
        "        ##############################YOUR CODE HERE############################\n",
        "        # 1. Calculate the size of the first portion of sf1 to be used.\n",
        "        #    This is a quarter of the number of channels (dimension 1).\n",
        "        # 2. Concatenate the first portion of sf1 with the second portion of sf2\n",
        "        # along the channel dimension.\n",
        "        #    - The first portion of sf1 should contain the first 'sf1_size' channels.\n",
        "        #    - The second portion of sf2 should contain the channels starting\n",
        "        #      from 'sf1_size' to the end.\n",
        "        ########################################################################\n",
        "        sf1_size = sf1.size(1) // 4\n",
        "        sf2_size = sf1.size(1) - sf1_size\n",
        "        sf1 = sf1[:, :sf1_size, :, :]\n",
        "        sf2 = sf2[:, sf1_size:, :, :]\n",
        "        sf = torch.cat((sf1, sf2), dim=1)\n",
        "        final_style_features[layer] = sf\n",
        "    return final_style_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfH9P49i6i89"
      },
      "outputs": [],
      "source": [
        "content_features = get_features(VGG19_pretrained, content_img)\n",
        "style_features1 = get_features(VGG19_pretrained, style_img1)\n",
        "style_features2 = get_features(VGG19_pretrained, style_img2)\n",
        "final_style_features = get_dual_style(style_features1, style_features2, style_layers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VgFr8cm6i89"
      },
      "source": [
        "## 4. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYFF__he6i89"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "target_img = content_img.clone().requires_grad_(True).to(device)\n",
        "optimizer = optim.Adam([target_img], lr=0.02)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nl1TMBv6NYat"
      },
      "outputs": [],
      "source": [
        "def style_tranfer_(model, optimizer, target_img,\n",
        "                   content_features, style_features,\n",
        "                   style_layers, content_weight, style_weight):\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    with torch.no_grad():\n",
        "        target_img.clamp_(0, 1)\n",
        "    target_features = get_features(model, target_img)\n",
        "\n",
        "    content_loss = ContentLoss(content_features['conv_4'],\n",
        "                               target_features['conv_4'])\n",
        "\n",
        "    style_loss = 0\n",
        "    for layer in style_layers:\n",
        "        target_gram = gram_matrix(target_features[layer])\n",
        "        style_gram = gram_matrix(style_features[layer])\n",
        "        style_loss += StyleLoss(style_gram, target_gram)\n",
        "\n",
        "    total_loss = content_loss*content_weight + style_loss*style_weight\n",
        "    total_loss.backward(retain_graph=True)\n",
        "    optimizer.step()\n",
        "    return total_loss, content_loss, style_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UllrF3V6i89",
        "outputId": "5b2f8b93-bbb3-4b1d-93ad-7d6944604810"
      },
      "outputs": [],
      "source": [
        "STEPS = 500\n",
        "\n",
        "for step in range(STEPS):\n",
        "    optimizer.zero_grad()\n",
        "    with torch.no_grad():\n",
        "        target_img.clamp_(0, 1)\n",
        "\n",
        "    total_loss, content_loss, style_loss = style_tranfer_(VGG19_pretrained, optimizer, target_img,\n",
        "                                                           content_features, final_style_features,\n",
        "                                                           style_layers, content_weight, style_weight)\n",
        "    if step % 100 == 99:\n",
        "        print(f\"Epoch [{step+1}/{STEPS}] Total loss: {total_loss.item():.6f} - \\\n",
        "                Content loss: {content_loss.item():.6f} - Style loss: {style_loss.item():.6f}\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        target_img.clamp_(0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "h5r1i6wU6i8-",
        "outputId": "5cece26b-7e51-4693-9bc6-7ce556223a91"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "imshow(target_img.detach(), title='Output Image')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
