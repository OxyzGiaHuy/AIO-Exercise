{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'ntc-scv'...\n",
      "Updating files:  72% (8/11)\n",
      "Updating files:  81% (9/11)\n",
      "Updating files:  90% (10/11)\n",
      "Updating files: 100% (11/11)\n",
      "Updating files: 100% (11/11), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/congnghia0609/ntc-scv.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files extracted to: ./data/ntc-scv\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "train_path = \"./ntc-scv/data/data_train.zip\"\n",
    "test_path = \"./ntc-scv/data/data_test.zip\"\n",
    "extract_path = \"./data/ntc-scv\"\n",
    "\n",
    "\n",
    "\n",
    "# Ensure the extraction folder exists\n",
    "os.makedirs(extract_path, exist_ok=True)\n",
    "\n",
    "# Unzipping the file\n",
    "with zipfile.ZipFile(train_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "\n",
    "with zipfile.ZipFile(test_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "\n",
    "print(f\"Files extracted to: {extract_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def load_data_from_path(folder_path):\n",
    "    examples = []\n",
    "    for label in os.listdir(folder_path):\n",
    "        full_path = os.path.join(folder_path, label)\n",
    "        for file_name in os.listdir(full_path):\n",
    "            file_path = os.path.join(full_path, file_name)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                lines = f.readlines()\n",
    "            sentence = \" \".join(lines)\n",
    "            if label == \"neg\":\n",
    "                label = 0\n",
    "            if label == \"pos\":\n",
    "                label = 1\n",
    "            data = {\n",
    "                'sentence': sentence,\n",
    "                'label': label\n",
    "            }\n",
    "            examples.append(data)\n",
    "    return pd.DataFrame(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_paths = {\n",
    "    'train': './data/ntc-scv/data_train/train',\n",
    "    'valid': './data/ntc-scv/data_train/test',\n",
    "    'test': './data/ntc-scv/data_test/test'\n",
    "}\n",
    "\n",
    "train_df = load_data_from_path(folder_paths['train'])\n",
    "valid_df = load_data_from_path(folder_paths['valid'])\n",
    "test_df = load_data_from_path(folder_paths['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langid\n",
      "  Downloading langid-1.1.6.tar.gz (1.9 MB)\n",
      "     ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "     ---------- ----------------------------- 0.5/1.9 MB 4.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.9/1.9 MB 7.6 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy in c:\\users\\admin\\anaconda3\\envs\\aio_exercise\\lib\\site-packages (from langid) (2.1.3)\n",
      "Building wheels for collected packages: langid\n",
      "  Building wheel for langid (setup.py): started\n",
      "  Building wheel for langid (setup.py): finished with status 'done'\n",
      "  Created wheel for langid: filename=langid-1.1.6-py3-none-any.whl size=1941178 sha256=fee57c8f6255e37447dcb71fcff00808d979404654eb44de613fee7c6b871f57\n",
      "  Stored in directory: c:\\users\\admin\\appdata\\local\\pip\\cache\\wheels\\23\\c8\\c6\\eed80894918490a175677414d40bd7c851413bbe03d4856c3c\n",
      "Successfully built langid\n",
      "Installing collected packages: langid\n",
      "Successfully installed langid-1.1.6\n"
     ]
    }
   ],
   "source": [
    "!pip install langid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langid.langid import LanguageIdentifier, model\n",
    "\n",
    "\n",
    "def identify_vn(df):\n",
    "    identifier = LanguageIdentifier.from_modelstring(model, norm_probs=True)\n",
    "    not_vi_idx = set()\n",
    "    THRESHOLD = 0.9\n",
    "    for idx, row in df.iterrows():\n",
    "        score = identifier.classify(row[\"sentence\"])\n",
    "        if score[0] != \"vi\" or (score[0] == \"vi\" and score[1] <= THRESHOLD):\n",
    "            not_vi_idx.add(idx)\n",
    "    vi_df = df[~df.index.isin(not_vi_idx)]\n",
    "    not_vi_df = df[df.index.isin(not_vi_idx)]\n",
    "    return vi_df, not_vi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # remove URLs https://www.\n",
    "    url_pattern = re.compile(r'https?://\\s+\\wwww\\.\\s+')\n",
    "    text = url_pattern.sub(r\" \", text)\n",
    "\n",
    "    # remove HTML Tags: <>\n",
    "    html_pattern = re.compile(r'<[^<>]+>')\n",
    "    text = html_pattern.sub(\" \", text)\n",
    "\n",
    "    # remove puncs and digits\n",
    "    replace_chars = list(string.punctuation + string.digits)\n",
    "    for char in replace_chars:\n",
    "        text = text.replace(char, \" \")\n",
    "\n",
    "    # remove emoji\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U0001F1F2-\\U0001F1F4\"  # Macau flag\n",
    "                               u\"\\U0001F1E6-\\U0001F1FF\"  # flags\n",
    "                               u\"\\U0001F600-\\U0001F64F\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U0001F1F2\"\n",
    "                               u\"\\U0001F1F4\"\n",
    "                               u\"\\U0001F620\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r\" \", text)\n",
    "\n",
    "    # normalize whitespace\n",
    "    text = \" \".join(text.split())\n",
    "\n",
    "    # lowercasing\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aio_exercise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
